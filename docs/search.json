[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/marks-and-channel/index.html",
    "href": "posts/marks-and-channel/index.html",
    "title": "Assignment 4: Marks and Channels",
    "section": "",
    "text": "Expressiveness and Effectiveness\n\nlibrary(ggplot2)\ndata(mtcars)\n\nggplot(mtcars, aes(x = mpg, y = hp, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(title = \"Proper Use of Marks and Channels\",\n       x = \"Miles per Gallon (mpg)\",\n       y = \"Horsepower (hp)\",\n       color = \"Cylinders\")\n\n\n\n\n\n\n\n\nCaption: Figure 1 shows the relationship between miles per gallon (mpg) and horsepower (hp) using color to distinguish between different numbers of cylinders. This adheres to the expressiveness and effectiveness principles.\n\nlibrary(datasets)\nlibrary(ggplot2)\nggplot(mtcars, aes(x = mpg, y = hp, size = factor(cyl), shape = factor(cyl), color = factor(cyl))) +\n  geom_point() +\n  labs(title = \"Violation of Marks and Channels\",\n       x = \"Miles per Gallon (mpg)\",\n       y = \"Horsepower (hp)\",\n       size = \"Cylinders\",\n       shape = \"Cylinders\",\n       color = \"Cylinders\")\n\nWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n\nCaption: Figure 2 uses size, shape, and color to encode the number of cylinders, violating the expressiveness and effectiveness principles. It creates visual clutter and confusion.\n\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(binwidth = 2, fill = \"skyblue\", color = \"white\") +\n  labs(title = \"Proper Discriminability\",\n       x = \"Miles per Gallon (mpg)\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\nCaption: Figure 3 uses an appropriate number of bins for the histogram, facilitating discriminability of the mpg attribute.\n\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(binwidth = 0.5, fill = \"skyblue\", color = \"white\") +\n  labs(title = \"Violation of Discriminability\",\n       x = \"Miles per Gallon (mpg)\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\nCaption: Figure 4 uses too many bins, violating the guidelines for discriminability and making it difficult to interpret the distribution of mpg.\n\nggplot(mtcars, aes(x = mpg, y = hp, color = factor(cyl), shape = factor(gear))) +\n  geom_point(size = 3) +\n  labs(title = \"Proper Separability\",\n       x = \"Miles per Gallon (mpg)\",\n       y = \"Horsepower (hp)\",\n       color = \"Cylinders\",\n       shape = \"Gears\")\n\n\n\n\n\n\n\n\nCaption: Figure 5 uses separate color and shape channels to encode the number of cylinders and gears, maintaining separability.\n\nggplot(mtcars, aes(x = mpg, y = hp, fill = factor(cyl), alpha = hp)) +\n  geom_point(size = 5, shape = 21) +\n  labs(title = \"Violation of Separability\",\n       x = \"Miles per Gallon (mpg)\",\n       y = \"Horsepower (hp)\",\n       fill = \"Cylinders\",\n       alpha = \"Horsepower\")\n\n\n\n\n\n\n\n\nCaption: Figure 6 uses fill color and transparency, which are less separable, to encode the number of cylinders and horsepower, violating the principle of separability.\n\nggplot(mtcars, aes(x = mpg, y = hp)) +\n  geom_point(size = 3, color = \"grey\") +\n  geom_point(data = subset(mtcars, cyl == 4), aes(x = mpg, y = hp), color = \"red\", size = 4) +\n  labs(title = \"Effective Popout\",\n       x = \"Miles per Gallon (mpg)\",\n       y = \"Horsepower (hp)\")\n\n\n\n\n\n\n\n\nCaption: Figure 7 highlights cars with 4 cylinders in red, effectively using the concept of popout.\n\nggplot(mtcars, aes(x = mpg, y = hp, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(title = \"Ineffective Popout\",\n       x = \"Miles per Gallon (mpg)\",\n       y = \"Horsepower (hp)\",\n       color = \"Cylinders\")\n\n\n\n\n\n\n\n\nCaption: Figure 8 uses color to distinguish all cylinder counts, making it difficult to identify specific points and failing to achieve the popout effect compared to Figure 7."
  },
  {
    "objectID": "posts/assignment-iii/index.html",
    "href": "posts/assignment-iii/index.html",
    "title": "Assignment 3: COVID-19 Strain Dynamics",
    "section": "",
    "text": "The covid_ts dataset tracks the daily counts of COVID-19 infections and deaths across U.S. states, broken down by different virus strains. It includes several key columns: the date, state, total infections, deaths, and daily case counts for each COVID-19 strain— Original, Alpha, Delta, and Omicron. The date column records the day of the entry, while the state column tells you which U.S. state the data pertains to. The total_infections column gives the overall count of COVID-19 cases, and deaths shows the number of lives lost to the virus. The strain-specific columns break down the daily cases for each variant: Original, Alpha, Delta, and Omicron.\nThe dataset begins in early 2020, during the first wave of the pandemic, and continues through the various waves caused by the emergence of new strains. These variants, like Alpha, Delta, and Omicron, had different levels of contagiousness and impacts on public health, which is reflected in their infection and death rates. For instance, the Alpha variant, emerging in late 2020, was more easily spread than the original strain, while Delta, which appeared in 2021, caused a major global surge. Omicron, emerging toward the end of 2021, was highly transmissible but led to fewer severe cases compared to previous strains.\nBy analyzing this dataset, we can track how each variant’s dominance shifted over time and across regions. It provides insight into when and where each strain became the most prevalent, shedding light on regional variations in how COVID-19 spread. Additionally, the dataset allows us to explore how mortality rates changed with each variant. By comparing death counts for each strain, we can gain a better understanding of how the severity of the virus evolved. Overall, the dataset is a powerful tool for both public health policy makers and epidemiological researchers. It allows us to visualize how the virus spread geographically and how different variants impacted different regions at various times. For example, using pie charts, we can easily show the proportion of each strain in different states, offering a clear picture of how the pandemic unfolded across the U.S. over time.\n\n# Print summary to verify strain distribution\nsummary_stats &lt;- covid_ts %&gt;%\n  group_by(state) %&gt;%\n  summarise(\n    total_cases = sum(total_infections, na.rm = TRUE),\n    total_original = sum(Original, na.rm = TRUE),\n    total_alpha = sum(Alpha, na.rm = TRUE),\n    total_delta = sum(Delta, na.rm = TRUE),\n    total_omicron = sum(Omicron, na.rm = TRUE)\n  )\nprint(summary_stats)\n\n# A tibble: 50 × 6\n   state       total_cases total_original total_alpha total_delta total_omicron\n   &lt;chr&gt;             &lt;int&gt;          &lt;int&gt;       &lt;int&gt;       &lt;int&gt;         &lt;int&gt;\n 1 Alabama          109073          66055       24671       13639          4708\n 2 Alaska           109220          65955       24546       13698          5021\n 3 Arizona          108820          65488       24718       13603          5011\n 4 Arkansas         109766          66186       24756       13912          4912\n 5 California       109730          66432       24537       13634          5127\n 6 Colorado         109579          66615       24612       13650          4702\n 7 Connecticut      110075          66941       24446       13823          4865\n 8 Delaware         109294          66325       24951       13421          4597\n 9 Florida          109710          66115       24964       13752          4879\n10 Georgia          109771          65969       25106       13906          4790\n# ℹ 40 more rows"
  },
  {
    "objectID": "posts/assignment-iii/index.html#description-of-data",
    "href": "posts/assignment-iii/index.html#description-of-data",
    "title": "Assignment 3: COVID-19 Strain Dynamics",
    "section": "",
    "text": "The covid_ts dataset tracks the daily counts of COVID-19 infections and deaths across U.S. states, broken down by different virus strains. It includes several key columns: the date, state, total infections, deaths, and daily case counts for each COVID-19 strain— Original, Alpha, Delta, and Omicron. The date column records the day of the entry, while the state column tells you which U.S. state the data pertains to. The total_infections column gives the overall count of COVID-19 cases, and deaths shows the number of lives lost to the virus. The strain-specific columns break down the daily cases for each variant: Original, Alpha, Delta, and Omicron.\nThe dataset begins in early 2020, during the first wave of the pandemic, and continues through the various waves caused by the emergence of new strains. These variants, like Alpha, Delta, and Omicron, had different levels of contagiousness and impacts on public health, which is reflected in their infection and death rates. For instance, the Alpha variant, emerging in late 2020, was more easily spread than the original strain, while Delta, which appeared in 2021, caused a major global surge. Omicron, emerging toward the end of 2021, was highly transmissible but led to fewer severe cases compared to previous strains.\nBy analyzing this dataset, we can track how each variant’s dominance shifted over time and across regions. It provides insight into when and where each strain became the most prevalent, shedding light on regional variations in how COVID-19 spread. Additionally, the dataset allows us to explore how mortality rates changed with each variant. By comparing death counts for each strain, we can gain a better understanding of how the severity of the virus evolved. Overall, the dataset is a powerful tool for both public health policy makers and epidemiological researchers. It allows us to visualize how the virus spread geographically and how different variants impacted different regions at various times. For example, using pie charts, we can easily show the proportion of each strain in different states, offering a clear picture of how the pandemic unfolded across the U.S. over time.\n\n# Print summary to verify strain distribution\nsummary_stats &lt;- covid_ts %&gt;%\n  group_by(state) %&gt;%\n  summarise(\n    total_cases = sum(total_infections, na.rm = TRUE),\n    total_original = sum(Original, na.rm = TRUE),\n    total_alpha = sum(Alpha, na.rm = TRUE),\n    total_delta = sum(Delta, na.rm = TRUE),\n    total_omicron = sum(Omicron, na.rm = TRUE)\n  )\nprint(summary_stats)\n\n# A tibble: 50 × 6\n   state       total_cases total_original total_alpha total_delta total_omicron\n   &lt;chr&gt;             &lt;int&gt;          &lt;int&gt;       &lt;int&gt;       &lt;int&gt;         &lt;int&gt;\n 1 Alabama          109073          66055       24671       13639          4708\n 2 Alaska           109220          65955       24546       13698          5021\n 3 Arizona          108820          65488       24718       13603          5011\n 4 Arkansas         109766          66186       24756       13912          4912\n 5 California       109730          66432       24537       13634          5127\n 6 Colorado         109579          66615       24612       13650          4702\n 7 Connecticut      110075          66941       24446       13823          4865\n 8 Delaware         109294          66325       24951       13421          4597\n 9 Florida          109710          66115       24964       13752          4879\n10 Georgia          109771          65969       25106       13906          4790\n# ℹ 40 more rows"
  },
  {
    "objectID": "posts/assignment-i/index.html",
    "href": "posts/assignment-i/index.html",
    "title": "Assignment 1: Anscombe’s Quartet",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\n\n\n# Load the dataset\ndata(anscombe)\n\n\n# Convert the data into a tidy format for ggplot\nanscombe_tidy &lt;- anscombe %&gt;%\n  pivot_longer(everything(), names_to = c(\".value\", \"set\"), names_pattern = \"(.)(.)\")\n\n\n# Plot the four datasets\nggplot(anscombe_tidy, aes(x = x, y = y)) +\n  geom_point(color = \"steelblue\", size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~set) +\n  theme_minimal() +\n  labs(title = \"Anscombe's Quartet\",\n       subtitle = \"Each dataset has nearly identical summary statistics\",\n       x = \"X\",\n       y = \"Y\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BCB520_Data_Visualization",
    "section": "",
    "text": "Welcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 10, 2025\n\n\nJustice Amenyo Kessie\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 1: Anscombe’s Quartet\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 24, 2025\n\n\nJustice Kessie\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 2: COVID-19 Data Visualization\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 24, 2025\n\n\nJustice Kessie\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 3: COVID-19 Strain Dynamics\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 24, 2025\n\n\nJustice Kessie\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 5: NHL Player Performance Analysis\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 24, 2025\n\n\nJustice Kessie\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 4: Marks and Channels\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 24, 2025\n\n\nJustice Kessie\n\n\n\n\n\n\n\n\n\n\n\n\nMidterm and Final Project\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 24, 2025\n\n\nJustice Kessie\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/assignment-ii/index.html",
    "href": "posts/assignment-ii/index.html",
    "title": "Assignment 2: COVID-19 Data Visualization",
    "section": "",
    "text": "The covid_ts dataset tracks the daily counts of COVID-19 infections and deaths across U.S. states, broken down by different virus strains. It includes several key columns: the date, state, total infections, deaths, and daily case counts for each COVID-19 strain— Original, Alpha, Delta, and Omicron. The date column records the day of the entry, while the state column tells you which U.S. state the data pertains to. The total_infections column gives the overall count of COVID-19 cases, and deaths shows the number of lives lost to the virus. The strain-specific columns break down the daily cases for each variant: Original, Alpha, Delta, and Omicron.\nThe dataset begins in early 2020, during the first wave of the pandemic, and continues through the various waves caused by the emergence of new strains. These variants, like Alpha, Delta, and Omicron, had different levels of contagiousness and impacts on public health, which is reflected in their infection and death rates. For instance, the Alpha variant, emerging in late 2020, was more easily spread than the original strain, while Delta, which appeared in 2021, caused a major global surge. Omicron, emerging toward the end of 2021, was highly transmissible but led to fewer severe cases compared to previous strains.\nBy analyzing this dataset, we can track how each variant’s dominance shifted over time and across regions. It provides insight into when and where each strain became the most prevalent, shedding light on regional variations in how COVID-19 spread. Additionally, the dataset allows us to explore how mortality rates changed with each variant. By comparing death counts for each strain, we can gain a better understanding of how the severity of the virus evolved. Overall, the dataset is a powerful tool for both public health policy makers and epidemiological researchers. It allows us to visualize how the virus spread geographically and how different variants impacted different regions at various times. For example, using pie charts, we can easily show the proportion of each strain in different states, offering a clear picture of how the pandemic unfolded across the U.S. over time.\n\n# Required packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nlibrary(gganimate)\nlibrary(gifski)\n\n\n# Print summary to verify strain distribution\nsummary_stats &lt;- covid_ts %&gt;%\n  group_by(state) %&gt;%\n  summarise(\n    total_cases = sum(total_infections, na.rm = TRUE),\n    total_original = sum(Original, na.rm = TRUE),\n    total_alpha = sum(Alpha, na.rm = TRUE),\n    total_delta = sum(Delta, na.rm = TRUE),\n    total_omicron = sum(Omicron, na.rm = TRUE)\n  )\nprint(summary_stats)\n\n# A tibble: 50 × 6\n   state       total_cases total_original total_alpha total_delta total_omicron\n   &lt;chr&gt;             &lt;int&gt;          &lt;int&gt;       &lt;int&gt;       &lt;int&gt;         &lt;int&gt;\n 1 Alabama          109539          65682       25021       14034          4802\n 2 Alaska           110052          66937       24660       13887          4568\n 3 Arizona          110436          67032       24881       13792          4731\n 4 Arkansas         109044          65642       24836       13768          4798\n 5 California       109176          66624       24302       13607          4643\n 6 Colorado         109372          66303       24677       13568          4824\n 7 Connecticut      109645          66791       24481       13669          4704\n 8 Delaware         109554          65733       25290       13836          4695\n 9 Florida          109953          66000       25072       13843          5038\n10 Georgia          109294          65753       24948       13955          4638\n# ℹ 40 more rows\n\n\n\n# Create the visualization with updated aesthetics\np &lt;- ggplot(map_data, aes(x = long, y = lat, group = piece_id)) +\n  geom_polygon(\n    aes(fill = strain),\n    color = \"white\",\n    linewidth = 0.2  # Updated from 'size' to 'linewidth'\n  ) +\n  coord_map(\"albers\", lat0 = 39, lat1 = 45) +\n  scale_fill_brewer(palette = \"Set3\") +\n  labs(\n    title = \"COVID-19 Strain Distribution in the United States\",\n    subtitle = \"Date: {frame_time}\",\n    fill = \"Dominant Strain\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12),\n    panel.grid = element_blank()  # Remove grid lines for cleaner map\n  ) +\n  transition_time(as.Date(paste(year, month, \"01\", sep = \"-\"))) +\n  ease_aes('linear')\n\n\np"
  },
  {
    "objectID": "posts/assignment-ii/index.html#description-of-data",
    "href": "posts/assignment-ii/index.html#description-of-data",
    "title": "Assignment 2: COVID-19 Data Visualization",
    "section": "",
    "text": "The covid_ts dataset tracks the daily counts of COVID-19 infections and deaths across U.S. states, broken down by different virus strains. It includes several key columns: the date, state, total infections, deaths, and daily case counts for each COVID-19 strain— Original, Alpha, Delta, and Omicron. The date column records the day of the entry, while the state column tells you which U.S. state the data pertains to. The total_infections column gives the overall count of COVID-19 cases, and deaths shows the number of lives lost to the virus. The strain-specific columns break down the daily cases for each variant: Original, Alpha, Delta, and Omicron.\nThe dataset begins in early 2020, during the first wave of the pandemic, and continues through the various waves caused by the emergence of new strains. These variants, like Alpha, Delta, and Omicron, had different levels of contagiousness and impacts on public health, which is reflected in their infection and death rates. For instance, the Alpha variant, emerging in late 2020, was more easily spread than the original strain, while Delta, which appeared in 2021, caused a major global surge. Omicron, emerging toward the end of 2021, was highly transmissible but led to fewer severe cases compared to previous strains.\nBy analyzing this dataset, we can track how each variant’s dominance shifted over time and across regions. It provides insight into when and where each strain became the most prevalent, shedding light on regional variations in how COVID-19 spread. Additionally, the dataset allows us to explore how mortality rates changed with each variant. By comparing death counts for each strain, we can gain a better understanding of how the severity of the virus evolved. Overall, the dataset is a powerful tool for both public health policy makers and epidemiological researchers. It allows us to visualize how the virus spread geographically and how different variants impacted different regions at various times. For example, using pie charts, we can easily show the proportion of each strain in different states, offering a clear picture of how the pandemic unfolded across the U.S. over time.\n\n# Required packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nlibrary(gganimate)\nlibrary(gifski)\n\n\n# Print summary to verify strain distribution\nsummary_stats &lt;- covid_ts %&gt;%\n  group_by(state) %&gt;%\n  summarise(\n    total_cases = sum(total_infections, na.rm = TRUE),\n    total_original = sum(Original, na.rm = TRUE),\n    total_alpha = sum(Alpha, na.rm = TRUE),\n    total_delta = sum(Delta, na.rm = TRUE),\n    total_omicron = sum(Omicron, na.rm = TRUE)\n  )\nprint(summary_stats)\n\n# A tibble: 50 × 6\n   state       total_cases total_original total_alpha total_delta total_omicron\n   &lt;chr&gt;             &lt;int&gt;          &lt;int&gt;       &lt;int&gt;       &lt;int&gt;         &lt;int&gt;\n 1 Alabama          109539          65682       25021       14034          4802\n 2 Alaska           110052          66937       24660       13887          4568\n 3 Arizona          110436          67032       24881       13792          4731\n 4 Arkansas         109044          65642       24836       13768          4798\n 5 California       109176          66624       24302       13607          4643\n 6 Colorado         109372          66303       24677       13568          4824\n 7 Connecticut      109645          66791       24481       13669          4704\n 8 Delaware         109554          65733       25290       13836          4695\n 9 Florida          109953          66000       25072       13843          5038\n10 Georgia          109294          65753       24948       13955          4638\n# ℹ 40 more rows\n\n\n\n# Create the visualization with updated aesthetics\np &lt;- ggplot(map_data, aes(x = long, y = lat, group = piece_id)) +\n  geom_polygon(\n    aes(fill = strain),\n    color = \"white\",\n    linewidth = 0.2  # Updated from 'size' to 'linewidth'\n  ) +\n  coord_map(\"albers\", lat0 = 39, lat1 = 45) +\n  scale_fill_brewer(palette = \"Set3\") +\n  labs(\n    title = \"COVID-19 Strain Distribution in the United States\",\n    subtitle = \"Date: {frame_time}\",\n    fill = \"Dominant Strain\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12),\n    panel.grid = element_blank()  # Remove grid lines for cleaner map\n  ) +\n  transition_time(as.Date(paste(year, month, \"01\", sep = \"-\"))) +\n  ease_aes('linear')\n\n\np"
  },
  {
    "objectID": "posts/assignment-v/index.html",
    "href": "posts/assignment-v/index.html",
    "title": "Assignment 5: NHL Player Performance Analysis",
    "section": "",
    "text": "# Load necessary libraries\nrequire(dplyr)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nrequire(ggplot2)\n\nLoading required package: ggplot2\n\nrequire(tidyr)\n\nLoading required package: tidyr\n\nrequire(lubridate)\n\nLoading required package: lubridate\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nrequire(tidyverse)\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats 1.0.0     ✔ stringr 1.5.1\n✔ purrr   1.0.2     ✔ tibble  3.2.1\n✔ readr   2.1.5     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nrequire(readr)"
  },
  {
    "objectID": "posts/assignment-v/index.html#hart-memorial-trophy-consideration-identifying-the-top-performing-nhl-players-to-their-respective-teams",
    "href": "posts/assignment-v/index.html#hart-memorial-trophy-consideration-identifying-the-top-performing-nhl-players-to-their-respective-teams",
    "title": "Assignment 5: NHL Player Performance Analysis",
    "section": "Hart Memorial Trophy Consideration: Identifying the Top-Performing NHL Players to their Respective Teams",
    "text": "Hart Memorial Trophy Consideration: Identifying the Top-Performing NHL Players to their Respective Teams\nTo identify the top-performing NHL players for the Hart Memorial Trophy consideration, we need to analyze player performance based on key metrics such as goals, assists, points, and plus-minus rating. The following R code snippet demonstrates how to load and analyze NHL player data to identify the top-performing players based on these metrics.\n\n# Load the dataset\nnhl_lines &lt;- read.csv(\"lines.csv\")\n\nTo do this, we created key metrics such as net value, net value per 60 minutes, team share, and “most valuable player” (MVP) score to evaluate player performance. We then filtered the dataset to include only players with a sufficient number of games played and calculated the MVP score for each player based on these metrics.\nMVP Score measures the player’s overall contribution to the team’s success, considering their performance in key areas such as goals, assists, and time on ice. The formula for MVP Score is a combination of net value per 60 minutes, time on ice, and team contribution percentage. The higher the MVP Score, the more valuable the player is to their team.\n\n# Results\nprint(top5[, c(\"name\", \"team\", \"position\", \"mvp_score\", \"netValue_per60\", \"icetime\", \"team_share\")])\n\n                   name team position mvp_score netValue_per60 icetime\n873     Ekholm-Bouchard  EDM  pairing 0.7051221     0.01650297   56099\n260        Slavin-Burns  CAR  pairing 0.5745015     0.01373800   59703\n587         Toews-Makar  COL  pairing 0.5134146     0.01251426   55233\n301 Gostisbehere-Walker  CAR  pairing 0.4464199     0.01812296   41086\n294     Orlov-Chatfield  CAR  pairing 0.4114806     0.01136221   55711\n    team_share\n873   18.03951\n260   17.11471\n587   17.45677\n301   12.15255\n294   15.34321\n\n\n\n# Top 10 MVP scores\nggplot(head(hart_ballot, 10), aes(x=reorder(name, mvp_score), y=mvp_score, fill=team)) +\n  geom_bar(stat=\"identity\") +\n  coord_flip() +\n  labs(title=\"Hart Trophy Candidates: MVP Score\", x=\"Player\", y=\"MVP Score\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe results shows that the pair Ekholm-Bouchard are the top-performing players based on the MVP score (0.705), followed by the triple Hyman-McDavid-Draisaitl for team Edmonton Oilers. The pairs Slavis-Burns, Gostisbehere-Walker, and Orlov-Chatfield also played an indispensable role in team CAR.\nMeanwhile the Hartman Memorial trophy goes to the pair Ekholm-Bouchard for their outstanding performance in the recent season."
  },
  {
    "objectID": "posts/assignment-v/index.html#vezina-trophy-consideration-identifying-the-top-performing-nhl-goalies",
    "href": "posts/assignment-v/index.html#vezina-trophy-consideration-identifying-the-top-performing-nhl-goalies",
    "title": "Assignment 5: NHL Player Performance Analysis",
    "section": "Vezina Trophy Consideration: Identifying the Top-Performing NHL Goalies",
    "text": "Vezina Trophy Consideration: Identifying the Top-Performing NHL Goalies\nTo find the best goalies in the NHL, we need to analyze their performance based on key metrics such as save percentage and goals against average (GAA). The following R code snippet demonstrates how to load and analyze NHL goalie data to identify the top-performing goalies based on these metrics for the Vezina Trophy consideration.\n\n# Load the dataset\nshots_data &lt;- read.csv(\"shots_2024.csv\")\n\nWe summarize the goalie performance based on the number of shots faced, goals allowed, saves made, save percentage, and games played. We then calculate the goals against average (GAA) for each goalie to further evaluate their performance.\n\ngoalie_stats &lt;- shots_data %&gt;%\n  filter(shotWasOnGoal == 1) %&gt;%  # Only consider shots on goal\n  group_by(goalieNameForShot) %&gt;%\n  summarise(\n    Shots_On_Goal = n(),\n    Goals_Allowed = sum(goal),\n    Saves = Shots_On_Goal - Goals_Allowed,\n    Save_Percentage = Saves / Shots_On_Goal,\n    Games_Played = n_distinct(game_id)\n  ) %&gt;%\n  arrange(desc(Save_Percentage))\n\n# View top goalies\nhead(goalie_stats)\n\n# A tibble: 6 × 6\n  goalieNameForShot Shots_On_Goal Goals_Allowed Saves Save_Percentage\n  &lt;chr&gt;                     &lt;int&gt;         &lt;int&gt; &lt;int&gt;           &lt;dbl&gt;\n1 Akira Schmid                 12             0    12           1    \n2 Nico Daws                    88             3    85           0.966\n3 Erik Portillo                29             1    28           0.966\n4 Marcus Hogberg              177            11   166           0.938\n5 Connor Hellebuyck          1395           102  1293           0.927\n6 Louis Domingue               27             2    25           0.926\n# ℹ 1 more variable: Games_Played &lt;int&gt;\n\n\nNext, we calculate the goals against average (GAA) for each goalie by dividing the total goals allowed by the total games played. We then merge the two datasets to compare the goalies based on save percentage and GAA.\n\ngoalie_gaa &lt;- shots_data %&gt;%\n  filter(shotWasOnGoal == 1) %&gt;%\n  group_by(goalieNameForShot) %&gt;%\n  summarise(\n    Goals_Allowed = sum(goal),\n    Games_Played = n_distinct(game_id),\n    GAA = (Goals_Allowed * 60) / (Games_Played * 60)  # Approximate per 60 minutes\n  ) %&gt;%\n  arrange(GAA)\n\n# View best GAA goalies\nhead(goalie_gaa)\n\n# A tibble: 6 × 4\n  goalieNameForShot Goals_Allowed Games_Played   GAA\n  &lt;chr&gt;                     &lt;int&gt;        &lt;int&gt; &lt;dbl&gt;\n1 \"Akira Schmid\"                0            1  0   \n2 \"Nico Daws\"                   3            4  0.75\n3 \"Erik Portillo\"               1            1  1   \n4 \"Yaniv Perets\"                1            1  1   \n5 \"\"                          418          395  1.06\n6 \"Marcus Hogberg\"             11            8  1.38\n\n\n\n## Merge the two datasets and delete the duplicate colummns\ngoalie_performance &lt;- merge(goalie_stats, goalie_gaa, by=\"goalieNameForShot\")\n\n# Rank by Save Percentage and GAA\ngoalie_performance &lt;- goalie_performance %&gt;%\n  arrange(desc(Save_Percentage), GAA)\n\n# View top-ranked goalies\nhead(goalie_performance)\n\n  goalieNameForShot Shots_On_Goal Goals_Allowed.x Saves Save_Percentage\n1      Akira Schmid            12               0    12       1.0000000\n2         Nico Daws            88               3    85       0.9659091\n3     Erik Portillo            29               1    28       0.9655172\n4    Marcus Hogberg           177              11   166       0.9378531\n5 Connor Hellebuyck          1395             102  1293       0.9268817\n6    Louis Domingue            27               2    25       0.9259259\n  Games_Played.x Goals_Allowed.y Games_Played.y      GAA\n1              1               0              1 0.000000\n2              4               3              4 0.750000\n3              1               1              1 1.000000\n4              8              11              8 1.375000\n5             52             102             52 1.961538\n6              1               2              1 2.000000\n\n\nTo filter out goalies who have faced a minimum number of shots and played a minimum number of games, we set thresholds for shots faced and games played. We then filter the dataset to include only goalies who meet these criteria. By the way, forgo the duplicate columns.\n\n# Set a threshold for minimum shots faced\nmin_shots &lt;- 1000\nmin_games_played &lt;- 50\n\n# Filter goalies who faced at least 'min_shots'\ngoalie_performance_filtered &lt;- goalie_performance %&gt;%\n  filter(Shots_On_Goal &gt;= min_shots & Games_Played.x &gt;= min_games_played) %&gt;%\n  arrange(desc(Save_Percentage), GAA)\n\n# View the updated rankings\nhead(goalie_performance_filtered)\n\n     goalieNameForShot Shots_On_Goal Goals_Allowed.x Saves Save_Percentage\n1    Connor Hellebuyck          1395             102  1293       0.9268817\n2   Andrei Vasilevskiy          1454             117  1337       0.9195323\n3         Ilya Sorokin          1393             131  1262       0.9059584\n4      Igor Shesterkin          1410             133  1277       0.9056738\n5 Ukko-Pekka Luukkonen          1350             151  1199       0.8881481\n  Games_Played.x Goals_Allowed.y Games_Played.y      GAA\n1             52             102             52 1.961538\n2             53             117             53 2.207547\n3             50             131             50 2.620000\n4             50             133             50 2.660000\n5             50             151             50 3.020000\n\n\nFinally, we create a scatter plot to visualize the relationship between save percentage and goals against average for the top-performing goalies. We use point size to represent the number of shots faced by each goalie. The plot provides a clear comparison of goalie performance based on these key metrics.\n\nggplot(goalie_performance_filtered, aes(x = Save_Percentage, y = GAA, label = goalieNameForShot, size = Shots_On_Goal)) +\n  geom_point(alpha = 0.8) +  # Semi-transparent points for better visualization\n  geom_text(vjust = -1, hjust = 0.5, size = 3.5, fontface = \"bold\") +  # Clearer labels\n  scale_size(range = c(3, 8)) +  # Adjust point sizes for better distinction\n  labs(\n    title = \"🏒 Goalie Performance: Save Percentage vs Goals Against Average\",\n    subtitle = paste(\"Minimum\", min_shots, \"shots faced required for inclusion\"),\n    x = \"Save Percentage (Higher is Better)\",\n    y = \"Goals Against Average (Lower is Better)\",\n    size = \"Shots Faced\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe adjudge Connor Hellebuyck, Andrei Vasilevskiy, and Ilya Sorokin as the top-3-performing goalies based on save percentage, goals against average and games played, with Connor being the best goaltender for the Vezina Trophy."
  },
  {
    "objectID": "posts/assignment-v/index.html#james-norris-memorial-trophy-consideration-identifying-the-best-all-around-defenseman",
    "href": "posts/assignment-v/index.html#james-norris-memorial-trophy-consideration-identifying-the-best-all-around-defenseman",
    "title": "Assignment 5: NHL Player Performance Analysis",
    "section": "James Norris Memorial Trophy Consideration: Identifying the “Best All-Around” Defenseman",
    "text": "James Norris Memorial Trophy Consideration: Identifying the “Best All-Around” Defenseman\nHere, we used the NHL Draft Stats dataset to identify the top defensemen based on key metrics such as points per game, assists per game, goals per game, offensive score, and defensive score. We then filtered the dataset to include only defensemen who played more than 50 games and ranked them based on their offensive and defensive scores.\n\n# Load the dataset\nNHLDraftStats &lt;- read.csv(\"SkaterIndividualstats.csv\")\n\nOffensive score is calculated as the sum of points per game, assists per game, and goals per game, while defensive score is calculated as the sum of shots blocked, hits, takeaways, and penalized by giveaways per game. We then rank the defensemen based on their offensive and defensive scores to identify the top performers.\n\n# Filter out defensemen\ndefensemen &lt;- NHLDraftStats %&gt;% filter(Position == \"D\")\n# Compute key metrics\ndefensemen &lt;- defensemen %&gt;%\n  mutate(\n    PPG = Total.Points / GP, # points per game\n    APG = Total.Assists / GP,# assists per game\n    GPG = Goals / GP,   # goals per game\n    Offensive.Score  = PPG + APG + GPG, # offensive score\n    Defensive.Score = (Shots.Blocked + Hits + Takeaways - Giveaways) / GP\n  )\n\n# Shortlisted to players who played more than 50 games\ndefensemen &lt;- defensemen %&gt;% filter(GP &gt; 50)\n\n# Rank players based on Defensive Score PPG and total points\ntop_defensemen &lt;- defensemen %&gt;% \n  arrange(desc(Defensive.Score + Offensive.Score)) %&gt;% \n  head(10)\n\nHere is a visualization of the top 5 defensemen based on their total points, combining offensive and defensive scores. The bar chart provides a clear comparison of the top defensemen based on their overall performance.\n\n# Bar chart of total points\nggplot(top_defensemen, aes(x = reorder(Player, -(Defensive.Score * Offensive.Score)), y = Defensive.Score * Offensive.Score, fill = Player)) +\n  geom_bar(stat = 'identity') +\n  labs(title = 'Top 5 Defensemen - Total Points', x = 'Player', y = 'Total Points') +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nWe ranked Moritz Seider, MacKenzie Weegar, Cale Makar, Colton Parayko and Neal Pionk as the top 5 defensemen based on their overall performance. Moritz Seider is the best defenseman for the James Norris Memorial Trophy."
  },
  {
    "objectID": "posts/assignment-v/index.html#calder-memorial-trophy-consideration-identifying-the-top-rookie-performer",
    "href": "posts/assignment-v/index.html#calder-memorial-trophy-consideration-identifying-the-top-rookie-performer",
    "title": "Assignment 5: NHL Player Performance Analysis",
    "section": "Calder Memorial Trophy Consideration: Identifying the Top Rookie Performer",
    "text": "Calder Memorial Trophy Consideration: Identifying the Top Rookie Performer\n\n# Load the dataset\nrookie_stats &lt;- read.csv(\"RookieSkaterIndividual.csv\")\n\n\n# View the first few rows\ncolnames(rookie_stats)\n\n [1] \"X\"                \"Player\"           \"Team\"             \"Position\"        \n [5] \"GP\"               \"TOI\"              \"Goals\"            \"Total.Assists\"   \n [9] \"First.Assists\"    \"Second.Assists\"   \"Total.Points\"     \"IPP\"             \n[13] \"Shots\"            \"SH.\"              \"ixG\"              \"iCF\"             \n[17] \"iFF\"              \"iSCF\"             \"iHDCF\"            \"Rush.Attempts\"   \n[21] \"Rebounds.Created\" \"PIM\"              \"Total.Penalties\"  \"Minor\"           \n[25] \"Major\"            \"Misconduct\"       \"Penalties.Drawn\"  \"Giveaways\"       \n[29] \"Takeaways\"        \"Hits\"             \"Hits.Taken\"       \"Shots.Blocked\"   \n[33] \"Faceoffs.Won\"     \"Faceoffs.Lost\"    \"Faceoffs..\"      \n\n\n\n# Compute key metrics\nrookies &lt;- rookie_stats %&gt;%\n  mutate(\n    PPG = Total.Points / GP,\n    APG = Total.Assists / GP,\n    GPG = Goals / GP,\n    Rookie.Score = (Total.Points + (Takeaways - Giveaways) + Shots + Hits) / GP,\n    PPGRS = PPG * Rookie.Score # combined metric of PPG and Rookie Score\n  )\n\n# Rank players based on overall Rookie Score\ntop_rookies &lt;- rookies %&gt;% \n  arrange(desc(Rookie.Score)) %&gt;% \n  head(10)\n\n# Ensure PPG is not missing or NA\ntop_rookies &lt;- top_rookies %&gt;% filter(!is.na(PPG))\n\n\nggplot(top_rookies, aes(x = reorder(Player, -PPGRS))) +\n  geom_bar(aes(y = PPGRS, fill = Player), stat = 'identity', alpha = 0.7) +\n  geom_line(aes(y = PPG * max(Rookie.Score, na.rm = TRUE) / max(PPG, na.rm = TRUE), group = 1), \n            color = 'blue', linewidth = 1) +\n  geom_point(aes(y = PPG * max(Rookie.Score, na.rm = TRUE) / max(PPG, na.rm = TRUE)), \n             color = 'blue', size = 3) +\n  scale_y_continuous(\n    name = 'Rookie Score', \n    sec.axis = sec_axis(~ . * max(top_rookies$PPG, na.rm = TRUE) / max(top_rookies$Rookie.Score, na.rm = TRUE), \n                        name = 'PPG')\n  ) +\n  labs(title = 'Top 5 Rookies - Rookie Score & PPG', x = 'Player') +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "posts/assignment-v/index.html#frank-j.-selke-trophy-consideration-identifying-the-best-defensive-forward",
    "href": "posts/assignment-v/index.html#frank-j.-selke-trophy-consideration-identifying-the-best-defensive-forward",
    "title": "Assignment 5: NHL Player Performance Analysis",
    "section": "Frank J. Selke Trophy Consideration: Identifying the Best Defensive Forward",
    "text": "Frank J. Selke Trophy Consideration: Identifying the Best Defensive Forward\n\n# Using the previous dataset\n# Filtering for forwards and calculating Selke Score\nforwards &lt;- rookie_stats %&gt;%\n  filter(Position != \"D\" & GP &gt;= 50) %&gt;%  \n  mutate(\n    Takeaway_Ratio = Takeaways / (Takeaways + Giveaways),\n    Blocked_Shot_Rate = Shots.Blocked / TOI * 60,\n    Faceoff_Effectiveness = ifelse(Faceoffs.Won + Faceoffs.Lost &gt; 0, \n                                  Faceoffs.Won / (Faceoffs.Won + Faceoffs.Lost), 0),\n    Penalty_Discipline = 1 - (PIM / (TOI/60)),\n    \n    # Combine into Selke Score\n    Selke_Score = (\n      (Takeaway_Ratio * 2.5) +          # Takeaways and giveaways\n      (Blocked_Shot_Rate * 1.5) +        # Blocking shots as a forward\n      (Faceoff_Effectiveness * 2) +      # Winning defensive zone faceoffs\n      (Penalty_Discipline * 1.5) +       # Staying out of the box\n      (Hits * 0.05) +                    # Physical play\n      (TOI / max(TOI, na.rm = TRUE) * 1.5)  # Coaches trust (ice time)\n    )\n  ) %&gt;%\n  # Scale the score for easier interpretation\n  mutate(Selke_Score = Selke_Score / max(Selke_Score, na.rm = TRUE) * 100)\nhead(forwards)\n\n    X           Player Team Position GP       TOI Goals Total.Assists\n1  41 Kirill Marchenko  CBJ        R 57 1071.6167    24            36\n2 145   Wyatt Johnston  DAL        C 60 1147.8833    21            34\n3  93      Marco Rossi  MIN        C 61 1117.2333    21            30\n4 115       JJ Peterka  BUF        R 57 1029.9500    17            30\n5 163     Logan Cooley  UTA        C 54  957.0333    17            30\n6  92   Dylan Holloway  STL        L 62 1031.9833    20            26\n  First.Assists Second.Assists Total.Points   IPP Shots   SH.   ixG iCF iFF\n1            23             13           60 64.52   155 15.48 17.56 323 226\n2            23             11           55 67.90   163 12.88 27.08 311 239\n3            17             13           51 69.86   102 20.59 15.52 198 155\n4            14             16           47 55.29   129 13.18 14.74 294 207\n5            15             15           47 67.14   115 14.78 12.16 219 164\n6            13             13           46 71.88   145 13.79 14.81 276 197\n  iSCF iHDCF Rush.Attempts Rebounds.Created PIM Total.Penalties Minor Major\n1  147    48             9               35  14               7     7     0\n2  225   116             4               35  12               6     6     0\n3  136    76             3               26  24              12    12     0\n4  131    43            12               20  24               8     7     0\n5  106    40             3               14  30              15    15     0\n6  139    57            10               24  10               5     5     0\n  Misconduct Penalties.Drawn Giveaways Takeaways Hits Hits.Taken Shots.Blocked\n1          0              13        65        29   43         67            29\n2          0              10        36        25   32         72            30\n3          0              16        42        14   42         49            28\n4          1              11        58        14   27         57             7\n5          0              13        48        20   40         94            24\n6          0               8        59        32  133         75            39\n  Faceoffs.Won Faceoffs.Lost Faceoffs.. Takeaway_Ratio Blocked_Shot_Rate\n1           11            24      31.43      0.3085106         1.6237149\n2          337           343      49.56      0.4098361         1.5681036\n3          349           388      47.35      0.2500000         1.5037145\n4            2             4      33.33      0.1944444         0.4077868\n5          241           298      44.71      0.2941176         1.5046498\n6           59            80      42.45      0.3516484         2.2674785\n  Faceoff_Effectiveness Penalty_Discipline Selke_Score\n1             0.3142857          0.2161376    49.79191\n2             0.4955882          0.3727586    51.83975\n3             0.4735414         -0.2888982    44.91144\n4             0.3333333         -0.3981261    24.94876\n5             0.4471243         -0.8808122    37.55984\n6             0.4244604          0.4185953    88.83601\n\n\n\n# View the top Selke candidates\ntop_selke_candidates &lt;- forwards %&gt;%\n  arrange(desc(Selke_Score)) %&gt;%\n  select(Player, Team, Position, GP, Selke_Score) %&gt;%\n  head(10)\n\ntop_selke_candidates &lt;- top_selke_candidates %&gt;%\n  mutate(\n    Expected_Selke = mean(Selke_Score),  # You can define your own expected value\n    Selke_Deviation = Selke_Score - Expected_Selke\n  )\n\n\n# Create a diverging bar chart\nggplot(top_selke_candidates, aes(x = reorder(Player, Selke_Score), y = Selke_Deviation, fill = Selke_Deviation &gt; 0)) +\n  geom_bar(stat = 'identity') +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\") +\n  scale_fill_manual(values = c(\"firebrick\", \"steelblue\"), \n                    labels = c(\"Below Expected\", \"Above Expected\"),\n                    name = \"\") +\n  labs(\n    title = 'Top Selke Candidates - Deviation from Expected Selke Score',\n    x = 'Player', \n    y = 'Deviation from Expected Selke Score'\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid.major.y = element_line(color = \"gray90\"),\n    panel.grid.minor = element_blank(),\n    legend.position = \"bottom\"\n  )"
  },
  {
    "objectID": "posts/assignment-v/index.html#lady-byng-memorial-trophy-consideration-identifying-the-most-sportsmanlike-player",
    "href": "posts/assignment-v/index.html#lady-byng-memorial-trophy-consideration-identifying-the-most-sportsmanlike-player",
    "title": "Assignment 5: NHL Player Performance Analysis",
    "section": "Lady Byng Memorial Trophy Consideration: Identifying the Most Sportsmanlike Player",
    "text": "Lady Byng Memorial Trophy Consideration: Identifying the Most Sportsmanlike Player\n\nSportsman &lt;- read.csv(\"SkaterIndividualstats.csv\")\n\n\ncolnames(Sportsman)\n\n [1] \"X\"                \"Player\"           \"Team\"             \"Position\"        \n [5] \"GP\"               \"TOI\"              \"Goals\"            \"Total.Assists\"   \n [9] \"First.Assists\"    \"Second.Assists\"   \"Total.Points\"     \"IPP\"             \n[13] \"Shots\"            \"SH.\"              \"ixG\"              \"iCF\"             \n[17] \"iFF\"              \"iSCF\"             \"iHDCF\"            \"Rush.Attempts\"   \n[21] \"Rebounds.Created\" \"PIM\"              \"Total.Penalties\"  \"Minor\"           \n[25] \"Major\"            \"Misconduct\"       \"Penalties.Drawn\"  \"Giveaways\"       \n[29] \"Takeaways\"        \"Hits\"             \"Hits.Taken\"       \"Shots.Blocked\"   \n[33] \"Faceoffs.Won\"     \"Faceoffs.Lost\"    \"Faceoffs..\"      \n\n\n\n## Compute key metrics\nSportsman &lt;- Sportsman %&gt;%\n  mutate(\n    PIM_Ratio = PIM / GP,  # Penalty minutes per game\n    PIM_Ratio = ifelse(is.na(PIM_Ratio), 0, PIM_Ratio),  # Replace NA with 0\n    PIM_Ratio = ifelse(PIM_Ratio &gt; 10, 10, PIM_Ratio),  # Cap at 10\n    PIM_Ratio = PIM_Ratio / 10,  # Scale to 0-1\n    PIM_Ratio = 1 - PIM_Ratio,  # Reverse scale\n    PIM_Ratio = PIM_Ratio * 100  # Scale to 0-100\n  )\n\nSportsman &lt;- Sportsman %&gt;%\n  filter(GP &gt;= 50) %&gt;%\n  mutate(\n    Sportsman_Score = (\n      (PIM_Ratio * 2) +  # Low penalty minutes\n      (Hits * 0.5) +     # Physical play\n      (Giveaways * 0.5)  # Few giveaways\n    )\n  )\n\ntop_sportsman &lt;- Sportsman %&gt;%\n  arrange(desc(Sportsman_Score)) %&gt;%\n  head(10)\n\n\n## Create a unique visualization\nggplot(top_sportsman, aes(x = reorder(Player, -Sportsman_Score), y = Sportsman_Score, fill = Sportsman_Score)) +\n  geom_bar(stat = 'identity') +\n  labs(title = 'Top 5 Sportsman Candidates - Sportsman Score', x = 'Player', y = 'Sportsman Score') +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "posts/midterm-and-final-project/index.html",
    "href": "posts/midterm-and-final-project/index.html",
    "title": "Midterm and Final Project",
    "section": "",
    "text": "Show the code\n# Install required packages if not already installed\nif (!require(\"leaflet\")) install.packages(\"leaflet\")\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\nif (!require(\"RColorBrewer\")) install.packages(\"RColorBrewer\")\nif (!require(\"htmlwidgets\")) install.packages(\"htmlwidgets\")\n\n# Load libraries\nlibrary(leaflet)\nlibrary(dplyr)\nlibrary(RColorBrewer)\nlibrary(htmlwidgets)\n\n# Read the data\nlassa_data &lt;- read.csv(\"lassa_virus_simulation_data.csv\")\n\n# Prepare data for map\nmap_data &lt;- lassa_data %&gt;%\n  group_by(country, city, lat, long) %&gt;%\n  summarize(\n    total_cases = n(),\n    human_cases = sum(host == \"human\"),\n    rodent_cases = sum(host == \"rodent\"),\n    avg_fitness = mean(fitness),\n    .groups = 'drop'\n  )\n\n# Create a color palette for the number of cases\npal &lt;- colorBin(\n  palette = \"YlOrRd\", \n  domain = map_data$total_cases,\n  bins = 7\n)\n\n# Create the map\nlassa_map &lt;- leaflet(map_data) %&gt;%\n  addTiles() %&gt;%  # Add default OpenStreetMap tiles\n  setView(lng = 0, lat = 8, zoom = 5) %&gt;%  # Center on West Africa\n  \n  # Add circle markers for each city\n  addCircleMarkers(\n    ~long, ~lat,\n    radius = ~sqrt(total_cases)/3,  # Size based on sqrt of cases\n    color = \"black\",\n    weight = 1,\n    fillColor = ~pal(total_cases),\n    fillOpacity = 0.7,\n    popup = ~paste(\n      \"&lt;strong&gt;\", city, \", \", country, \"&lt;/strong&gt;&lt;br&gt;\",\n      \"Total cases: \", total_cases, \"&lt;br&gt;\",\n      \"Human cases: \", human_cases, \"&lt;br&gt;\",\n      \"Rodent cases: \", rodent_cases, \"&lt;br&gt;\",\n      \"Avg. fitness: \", round(avg_fitness, 2)\n    )\n  ) %&gt;%\n  \n  # Add a legend\n  addLegend(\n    position = \"bottomright\",\n    pal = pal,\n    values = ~total_cases,\n    title = \"Total Cases\",\n    opacity = 0.7\n  )\n\n# Save the map\nsaveWidget(lassa_map, \"lassa_geographic_map.html\", selfcontained = TRUE)\n\n# Display the map\nlassa_map\n\n\n\n\n\n\n\n\nShow the code\n# Install required packages\nif (!require(\"networkD3\")) install.packages(\"networkD3\")\n\n\nLoading required package: networkD3\n\n\n\nAttaching package: 'networkD3'\n\n\nThe following object is masked from 'package:htmlwidgets':\n\n    JS\n\n\nThe following object is masked from 'package:leaflet':\n\n    JS\n\n\nShow the code\nif (!require(\"igraph\")) install.packages(\"igraph\")\n\n# Load libraries\nlibrary(networkD3)\nlibrary(igraph)\nlibrary(dplyr)\n\n# Prepare data for the network visualization\n# We'll create a sample of transmissions for clarity\nset.seed(123) # For reproducibility\n\n# Get unique strains and clades\nstrains &lt;- unique(lassa_data$strain)\ntransmission_sample &lt;- lassa_data %&gt;%\n  filter(transmission %in% c(\"rodent-human\", \"human-human\")) %&gt;%\n  sample_n(min(500, nrow(.)))  # Take a sample to avoid overcrowding\n\n# Create edges (from-to connections)\n# For human-human transmissions, we'll simulate connections based on generation sequence\nedges &lt;- data.frame(source = character(), target = character(), value = numeric(), type = character())\n\nfor (strain in strains) {\n  strain_data &lt;- transmission_sample %&gt;% filter(strain == !!strain)\n  \n  if (nrow(strain_data) &gt; 1) {\n    # Sort by generation\n    strain_data &lt;- strain_data %&gt;% arrange(generation)\n    \n    # Create connections between consecutive generations\n    for (i in 1:(nrow(strain_data)-1)) {\n      # Only connect if they could be related (later generation)\n      if (strain_data$generation[i+1] &gt; strain_data$generation[i]) {\n        edges &lt;- rbind(edges, data.frame(\n          source = strain_data$clade[i],\n          target = strain_data$clade[i+1],\n          value = 1,\n          type = strain_data$transmission[i+1]\n        ))\n      }\n    }\n  }\n}\n\n# Create nodes dataframe\nnodes &lt;- transmission_sample %&gt;%\n  select(clade, strain, host, fitness) %&gt;%\n  distinct() %&gt;%\n  mutate(group = as.integer(factor(strain)))\n\n# Match node indices for the network\nedges$source_id &lt;- match(edges$source, nodes$clade) - 1\nedges$target_id &lt;- match(edges$target, nodes$clade) - 1\n\n# Filter out any NA relationships\nedges &lt;- edges %&gt;% filter(!is.na(source_id) & !is.na(target_id))\n\n# Create the force network\nforce_network &lt;- forceNetwork(\n  Links = edges, \n  Nodes = nodes,\n  Source = \"source_id\",\n  Target = \"target_id\",\n  NodeID = \"clade\",\n  Group = \"group\",\n  Value = \"value\",\n  Nodesize = \"fitness\",\n  opacity = 0.8,\n  linkColour = ifelse(edges$type == \"human-human\", \"#E69F00\", \"#56B4E9\"),\n  zoom = TRUE,\n  legend = TRUE,\n  bounded = TRUE,\n  fontSize = 12,\n  charge = -50\n)\n\n# Save the network visualization\nsaveWidget(force_network, \"lassa_transmission_network.html\", selfcontained = TRUE)\n\n# Display the network\nforce_network\n\n\n\n\n\n\n\n\nShow the code\n# Install required packages\nif (!require(\"plotly\")) install.packages(\"plotly\")\n\n# Load libraries\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(lubridate)\n\n# Convert date strings to Date objects\nlassa_data$date &lt;- as.Date(lassa_data$date)\n\n# Create a sample for better visualization\nset.seed(42)\nspatial_sample &lt;- lassa_data %&gt;%\n  arrange(date) %&gt;%\n  sample_n(min(3000, nrow(.)))\n\n# Create hover text\nspatial_sample$hover_text &lt;- paste(\n  \"Date:\", spatial_sample$date, \"&lt;br&gt;\",\n  \"Location:\", spatial_sample$city, \",\", spatial_sample$country, \"&lt;br&gt;\",\n  \"Host:\", spatial_sample$host, \"&lt;br&gt;\",\n  \"Strain:\", spatial_sample$strain, \"&lt;br&gt;\",\n  \"Transmission:\", spatial_sample$transmission\n)\n\n# Create the spatial plot\nspatial_plot &lt;- plot_ly(\n  data = spatial_sample,\n  x = ~long, \n  y = ~lat,\n  z = ~as.numeric(date - min(date)) / 7,  # Convert to weeks since first case\n  type = \"scatter3d\",\n  mode = \"markers\",\n  marker = list(\n    size = 5,\n    color = ~as.numeric(date),  # Color by date\n    colorscale = \"Viridis\",\n    opacity = 0.7\n  ),\n  text = ~hover_text,\n  hoverinfo = \"text\",\n  color = ~as.factor(host)\n) %&gt;%\n  layout(\n    title = \"Spatial-Temporal Spread of Lassa Virus\",\n    scene = list(\n      xaxis = list(title = \"Longitude\"),\n      yaxis = list(title = \"Latitude\"),\n      zaxis = list(title = \"Weeks Since First Case\")\n    ),\n    legend = list(title = list(text = \"Host\"))\n  ) %&gt;%\n  colorbar(title = \"Date\")\n\n\nWarning in RColorBrewer::brewer.pal(N, \"Set2\"): minimal value for n is 3, returning requested palette with 3 different levels\nWarning in RColorBrewer::brewer.pal(N, \"Set2\"): minimal value for n is 3, returning requested palette with 3 different levels\n\n\nWarning: Didn't find a colorbar to modify.\n\n\nShow the code\n# Save the plot\nhtmlwidgets::saveWidget(\n  as_widget(spatial_plot),\n  \"lassa_spatial_temporal_spread.html\",\n  selfcontained = TRUE\n)\n\n# Display the plot\nspatial_plot\n\n\n\n\n\n\n\n\nShow the code\n# Install required packages\nif (!require(\"plotly\")) install.packages(\"plotly\")\nif (!require(\"tidyr\")) install.packages(\"tidyr\")\n\n# Load libraries\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(tidyr)\n\n# Prepare data for strain evolution plot\nevolution_data &lt;- lassa_data %&gt;%\n  group_by(strain, date = as.Date(date)) %&gt;%\n  summarize(\n    cases = n(),\n    avg_fitness = mean(fitness),\n    human_cases = sum(host == \"human\"),\n    rodent_cases = sum(host == \"rodent\"),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(date)\n\n# Create the strain evolution plot\nevolution_plot &lt;- plot_ly() %&gt;%\n  layout(\n    title = \"Evolution of Lassa Virus Strains Over Time\",\n    xaxis = list(title = \"Date\"),\n    yaxis = list(title = \"Number of Cases\"),\n    hovermode = \"closest\",\n    legend = list(title = list(text = \"Virus Strain\"))\n  )\n\n# Add each strain as a trace\nfor (strain in unique(evolution_data$strain)) {\n  strain_data &lt;- evolution_data %&gt;% filter(strain == !!strain)\n  \n  evolution_plot &lt;- evolution_plot %&gt;%\n    add_trace(\n      data = strain_data,\n      x = ~date,\n      y = ~cases,\n      type = \"scatter\",\n      mode = \"lines+markers\",\n      name = strain,\n      line = list(shape = \"spline\", smoothing = 0.3),\n      marker = list(\n        size = ~sqrt(avg_fitness) * 5,\n        opacity = 0.7\n      ),\n      hoverinfo = \"text\",\n      text = ~paste(\n        \"Strain:\", strain, \"&lt;br&gt;\",\n        \"Date:\", date, \"&lt;br&gt;\",\n        \"Total Cases:\", cases, \"&lt;br&gt;\",\n        \"Human Cases:\", human_cases, \"&lt;br&gt;\",\n        \"Rodent Cases:\", rodent_cases, \"&lt;br&gt;\",\n        \"Avg Fitness:\", round(avg_fitness, 2)\n      )\n    )\n}\n\n# Add fitness as a secondary y-axis\nevolution_plot_fitness &lt;- evolution_data %&gt;%\n  group_by(date, strain) %&gt;%\n  summarize(\n    avg_fitness = mean(avg_fitness),\n    .groups = 'drop'\n  ) %&gt;%\n  pivot_wider(\n    names_from = strain,\n    values_from = avg_fitness,\n    values_fill = list(avg_fitness = NA)\n  ) %&gt;%\n  plot_ly() %&gt;%\n  layout(\n    title = \"Fitness Evolution of Lassa Virus Strains\",\n    xaxis = list(title = \"Date\"),\n    yaxis = list(title = \"Fitness (R0)\"),\n    hovermode = \"closest\"\n  )\n\nfor (strain in unique(evolution_data$strain)) {\n  strain_data &lt;- evolution_data %&gt;% \n    filter(strain == !!strain) %&gt;%\n    arrange(date)\n  \n  if (nrow(strain_data) &gt; 0) {\n    evolution_plot_fitness &lt;- evolution_plot_fitness %&gt;%\n      add_trace(\n        x = strain_data$date,\n        y = strain_data$avg_fitness,\n        type = \"scatter\",\n        mode = \"lines\",\n        name = strain,\n        line = list(shape = \"spline\", smoothing = 0.3)\n      )\n  }\n}\n\n# Create a subplot combining cases and fitness\ncombined_evolution_plot &lt;- subplot(\n  evolution_plot, evolution_plot_fitness,\n  nrows = 2,\n  heights = c(0.6, 0.4),\n  shareX = TRUE\n)\n\n# Save the evolution plot\nhtmlwidgets::saveWidget(\n  as_widget(combined_evolution_plot),\n  \"lassa_strain_evolution.html\",\n  selfcontained = TRUE\n)\n\n# Display the combined plot\ncombined_evolution_plot\n\n\n\n\n\n\n\n\nShow the code\n# Install required packages\nif (!require(\"plotly\")) install.packages(\"plotly\")\nif (!require(\"lubridate\")) install.packages(\"lubridate\")\n\n# Load libraries\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(lubridate)\n\n# Prepare data for calendar heatmap\nlassa_data$date &lt;- as.Date(lassa_data$date)\n\n# Aggregate by day and host type\ncalendar_data &lt;- lassa_data %&gt;%\n  group_by(date, host) %&gt;%\n  summarize(\n    cases = n(),\n    .groups = 'drop'\n  ) %&gt;%\n  tidyr::complete(\n    date = seq.Date(min(lassa_data$date), max(lassa_data$date), by = \"day\"),\n    host = c(\"human\", \"rodent\"),\n    fill = list(cases = 0)\n  ) %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    day = day(date),\n    weekday = wday(date),\n    week = week(date)\n  )\n\n# Create a heatmap for human cases\nhuman_heatmap &lt;- calendar_data %&gt;%\n  filter(host == \"human\") %&gt;%\n  plot_ly(\n    x = ~weekday,\n    y = ~week,\n    z = ~cases,\n    type = \"heatmap\",\n    colorscale = \"YlOrRd\",\n    hoverinfo = \"text\",\n    text = ~paste(\n      \"Date:\", date, \"&lt;br&gt;\",\n      \"Human Cases:\", cases\n    )\n  ) %&gt;%\n  layout(\n    title = \"Human Lassa Virus Cases by Day\",\n    xaxis = list(\n      title = \"\",\n      tickvals = 1:7,\n      ticktext = c(\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\")\n    ),\n    yaxis = list(\n      title = \"Week Number\",\n      autorange = \"reversed\"  # To have week 1 at the top\n    )\n  )\n\n# Create a heatmap for rodent cases\nrodent_heatmap &lt;- calendar_data %&gt;%\n  filter(host == \"rodent\") %&gt;%\n  plot_ly(\n    x = ~weekday,\n    y = ~week,\n    z = ~cases,\n    type = \"heatmap\",\n    colorscale = \"Blues\",\n    hoverinfo = \"text\",\n    text = ~paste(\n      \"Date:\", date, \"&lt;br&gt;\",\n      \"Rodent Cases:\", cases\n    )\n  ) %&gt;%\n  layout(\n    title = \"Rodent Lassa Virus Cases by Day\",\n    xaxis = list(\n      title = \"\",\n      tickvals = 1:7,\n      ticktext = c(\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\")\n    ),\n    yaxis = list(\n      title = \"Week Number\",\n      autorange = \"reversed\"  # To have week 1 at the top\n    )\n  )\n\n# Combine into a subplot\ncalendar_plot &lt;- subplot(\n  human_heatmap, rodent_heatmap,\n  nrows = 2,\n  heights = c(0.5, 0.5),\n  shareX = TRUE\n) %&gt;%\n  layout(\n    title = \"Calendar Heatmap of Lassa Virus Cases\"\n  )\n\n# Save the calendar heatmap\nhtmlwidgets::saveWidget(\n  as_widget(calendar_plot),\n  \"lassa_calendar_heatmap.html\",\n  selfcontained = TRUE\n)\n\n# Display the heatmap\ncalendar_plot\n\n\n\n\n\n\n\n\nShow the code\n# Install required packages\nif (!require(\"igraph\")) install.packages(\"igraph\")\nif (!require(\"ggraph\")) install.packages(\"ggraph\")\n\n\nLoading required package: ggraph\n\n\nShow the code\nif (!require(\"tidygraph\")) install.packages(\"tidygraph\")\n\n\nLoading required package: tidygraph\n\n\n\nAttaching package: 'tidygraph'\n\n\nThe following object is masked from 'package:igraph':\n\n    groups\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nShow the code\nif (!require(\"viridis\")) install.packages(\"viridis\")\n\n\nLoading required package: viridis\n\n\nLoading required package: viridisLite\n\n\nShow the code\nif (!require(\"htmlwidgets\")) install.packages(\"htmlwidgets\")\nif (!require(\"plotly\")) install.packages(\"plotly\")\n\n# Load libraries\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(tidygraph)\nlibrary(dplyr)\nlibrary(viridis)\nlibrary(htmlwidgets)\nlibrary(plotly)\n\n# Read the data\nlassa_data &lt;- read.csv(\"lassa_virus_simulation_data.csv\")\n\n# Prepare data for the network visualization\nset.seed(42) # For reproducibility\n\n# Create a subset of clades for clearer visualization\n# Sample based on strain to ensure all strains are represented\nstrains &lt;- unique(lassa_data$strain)\nclade_sample &lt;- data.frame()\n\nfor (strain in strains) {\n  # Get data for this strain\n  strain_data &lt;- lassa_data %&gt;% \n    filter(strain == !!strain) %&gt;%\n    distinct(clade, .keep_all = TRUE)\n  \n  # Sample clades from this strain (adjust sample size for visual clarity)\n  sample_size &lt;- min(150, nrow(strain_data))\n  strain_sample &lt;- strain_data %&gt;% sample_n(sample_size)\n  \n  # Add to our overall sample\n  clade_sample &lt;- rbind(clade_sample, strain_sample)\n}\n\n# Create nodes dataframe\nnodes &lt;- clade_sample %&gt;%\n  select(clade, strain, host, fitness) %&gt;%\n  distinct()\n\n# Create edges based on genetic similarity and transmission patterns\nedges &lt;- data.frame(from = character(), to = character(), weight = numeric())\n\n# For each strain, create connections between clades\nfor (strain in unique(nodes$strain)) {\n  strain_nodes &lt;- nodes %&gt;% filter(strain == !!strain)\n  \n  if (nrow(strain_nodes) &gt; 1) {\n    # Create a sample of connections within the strain\n    n_connections &lt;- min(nrow(strain_nodes) * 2, 200)  # Limit connections for visual clarity\n    \n    for (i in 1:n_connections) {\n      # Select two random nodes from the strain\n      selected_nodes &lt;- strain_nodes %&gt;% sample_n(2)\n      \n      # Add edge with weight based on similarity (inverse of fitness difference)\n      fitness_diff &lt;- abs(selected_nodes$fitness[1] - selected_nodes$fitness[2])\n      weight &lt;- 1 / (fitness_diff + 0.1)  # Add small constant to avoid division by zero\n      \n      edges &lt;- rbind(edges, data.frame(\n        from = selected_nodes$clade[1],\n        to = selected_nodes$clade[2],\n        weight = weight\n      ))\n    }\n  }\n}\n\n# Create a few connections between strains to show potential recombination/mutation events\nfor (i in 1:10) {\n  # Select two random nodes from different strains\n  strain1 &lt;- sample(strains, 1)\n  strain2 &lt;- sample(strains[strains != strain1], 1)\n  \n  node1 &lt;- nodes %&gt;% filter(strain == strain1) %&gt;% sample_n(1)\n  node2 &lt;- nodes %&gt;% filter(strain == strain2) %&gt;% sample_n(1)\n  \n  # Add edge with lower weight (represents less common transition)\n  edges &lt;- rbind(edges, data.frame(\n    from = node1$clade,\n    to = node2$clade,\n    weight = 0.2  # Lower weight for inter-strain connections\n  ))\n}\n\n# Create graph object\ngraph &lt;- graph_from_data_frame(d = edges, vertices = nodes, directed = FALSE)\n\n# Add node attributes\nV(graph)$color &lt;- as.factor(V(graph)$strain)\nV(graph)$size &lt;- V(graph)$fitness * 2\n\n# Use the Fruchterman-Reingold layout for better clustering\nlayout &lt;- layout_with_fr(graph, niter = 1000)\n\n# Create a static plot first to see the structure\nstatic_plot &lt;- ggraph(graph, layout = \"fr\") + \n  geom_edge_link(aes(alpha = weight), color = \"lightgray\", width = 0.5) +\n  geom_node_point(aes(color = strain, size = fitness)) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme_graph() +\n  labs(title = \"Lassa Virus Transmission Network\") +\n  theme(legend.position = \"right\")\n\n# Save the layout coordinates for use in the interactive plot\ncoords &lt;- layout_with_fr(graph, niter = 1000)\n\n# Create node information for interactive plot\nnode_info &lt;- as_data_frame(graph, what = \"vertices\")\nnode_info$x &lt;- coords[,1]\nnode_info$y &lt;- coords[,2]\nnode_info$id &lt;- 1:nrow(node_info)\n\n# Create edge information\nedge_info &lt;- as_data_frame(graph, what = \"edges\")\nedge_info$id &lt;- 1:nrow(edge_info)\n\n# Add source and target IDs based on node IDs\nfrom_indices &lt;- match(edge_info$from, node_info$clade)\nto_indices &lt;- match(edge_info$to, node_info$clade)\n\n# Create a color mapping for the strains\nstrain_colors &lt;- c(\"#E41A1C\", \"#FF7F00\", \"#377EB8\", \"#4DAF4A\", \"#984EA3\", \"#FFFF33\")\nnames(strain_colors) &lt;- sort(unique(node_info$strain))\n\n# Create the interactive plot with plotly\nnetwork_plot &lt;- plot_ly() %&gt;%\n  # Add edges\n  add_trace(\n    type = \"scatter\",\n    mode = \"lines\",\n    x = c(rbind(node_info$x[from_indices], node_info$x[to_indices], rep(NA, length(from_indices)))),\n    y = c(rbind(node_info$y[from_indices], node_info$y[to_indices], rep(NA, length(from_indices)))),\n    line = list(color = \"rgba(190, 190, 190, 0.3)\", width = 0.5),\n    hoverinfo = \"none\",\n    showlegend = FALSE\n  ) %&gt;%\n  # Add nodes\n  add_trace(\n    type = \"scatter\",\n    mode = \"markers\",\n    x = node_info$x,\n    y = node_info$y,\n    marker = list(\n      size = 8 + node_info$fitness * 2,\n      color = unname(strain_colors[node_info$strain]),\n      line = list(color = \"#FFFFFF\", width = 0.5)\n    ),\n    text = paste(\n      \"Clade:\", node_info$clade, \"&lt;br&gt;\",\n      \"Strain:\", node_info$strain, \"&lt;br&gt;\",\n      \"Host:\", node_info$host, \"&lt;br&gt;\",\n      \"Fitness:\", round(node_info$fitness, 2)\n    ),\n    hoverinfo = \"text\",\n    showlegend = FALSE\n  )\n\n# Add traces for the legend\nfor (strain in unique(node_info$strain)) {\n  network_plot &lt;- network_plot %&gt;%\n    add_trace(\n      type = \"scatter\",\n      mode = \"markers\",\n      x = c(NA),\n      y = c(NA),\n      marker = list(size = 10, color = strain_colors[strain]),\n      name = strain,\n      showlegend = TRUE\n    )\n}\n\n# Update layout\nnetwork_plot &lt;- network_plot %&gt;% layout(\n  title = \"Lassa Virus Transmission Network\",\n  xaxis = list(title = \"\", showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),\n  yaxis = list(title = \"\", showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),\n  hovermode = \"closest\",\n  legend = list(title = list(text = \"Strains\")),\n  margin = list(l = 0, r = 0, b = 0, t = 50)\n)\n\n# Save the plot\nhtmlwidgets::saveWidget(\n  as_widget(network_plot),\n  \"lassa_transmission_network.html\",\n  selfcontained = TRUE\n)\n\n\nWarning: Ignoring 1 observations\nWarning: Ignoring 1 observations\nWarning: Ignoring 1 observations\nWarning: Ignoring 1 observations\nWarning: Ignoring 1 observations\n\n\nShow the code\n# Display the plot\nnetwork_plot\n\n\nWarning: Ignoring 1 observations\nWarning: Ignoring 1 observations\nWarning: Ignoring 1 observations\nWarning: Ignoring 1 observations\nWarning: Ignoring 1 observations"
  }
]